{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import wave\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiseAssistentPipeline:\n",
    "\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "\n",
    "        self.device = device\n",
    "        self.model_id = \"openai/whisper-large-v3\"\n",
    "        self.torch_dtype = torch.float32\n",
    "\n",
    "        self.transcribe_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            self.model_id,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True).to(self.device)\n",
    "\n",
    "        self.transcribe_model_processor = AutoProcessor.from_pretrained(self.model_id)\n",
    "\n",
    "        self.pipeline = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model= self.transcribe_model,\n",
    "            tokenizer=self.transcribe_model_processor.tokenizer,\n",
    "            feature_extractor=self.transcribe_model_processor.feature_extractor,\n",
    "            max_new_tokens=128,\n",
    "            chunk_length_s=30,\n",
    "            batch_size=16,\n",
    "            return_timestamps=True,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        self.similarity_model = SentenceTransformer(\n",
    "            'sentence-transformers/LaBSE', device=self.device)\n",
    "\n",
    "    def voise_record(self, chank=1024,\n",
    "                     audio_format=pyaudio.paInt16,\n",
    "                     channels=1,\n",
    "                     rate=16000,\n",
    "                     duration=15,\n",
    "                     output_filename=\"output.wav\"):\n",
    "\n",
    "        p = pyaudio.PyAudio()\n",
    "\n",
    "        stream = p.open(format=audio_format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chank)\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        for i in range(0, int(rate / chank * duration)):\n",
    "            data = stream.read(chank)\n",
    "            frames.append(data)\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        wf = wave.open(output_filename, 'wb')\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(p.get_sample_size(audio_format))\n",
    "        wf.setframerate(rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "\n",
    "    \n",
    "    def transcribe_audio(self, audio):\n",
    "        result = self.pipeline(audio, generate_kwargs={\"language\": \"russian\"})\n",
    "        return result[\"text\"]\n",
    "\n",
    "    def create_embedding(self, origin_sentence):\n",
    "        embedding = self.similarity_model.encode(origin_sentence, convert_to_tensor=True)\n",
    "        return embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = \"./test_output.wav\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "origin_sentence = \"Не включилось РУ6\"\n",
    "sentences = [\n",
    "    'Не включилось РУ6',\n",
    "    'РУ6 не включилось',\n",
    "    'не включилось шестое реле управления',\n",
    "    'Реле РУ6 срабатывает, но не включается реле времени РВ1, РВ2',\n",
    "    'При нажатии кнопки \"Пуск дизеля\" (все нужные автоматы включены) КМН не включается.',\n",
    "    'При нажатии кнопки \"Пуск дизеля\" контактор КМН включается, но маслопрокачивающий насос не работает',\n",
    "    'При пуске прокачка масла есть (60-90 сек), но после отключения КМН пусковые контакторы не включаются',\n",
    "    'При нажатии кнопки \"ПД\" включаются пусковые контакторы без предварительной прокачки масла'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "assistent = VoiseAssistentPipeline(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistent.voise_record(duration=4, output_filename=AUDIO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, _ = librosa.load(AUDIO_PATH, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' не включил шестой реле управления.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = assistent.transcribe_audio(audio)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6121]])\n",
      "tensor([[0.6537]])\n",
      "tensor([[0.9444]])\n",
      "tensor([[0.5294]])\n",
      "tensor([[0.4407]])\n",
      "tensor([[0.3804]])\n",
      "tensor([[0.3741]])\n",
      "tensor([[0.3401]])\n"
     ]
    }
   ],
   "source": [
    "origin_embedding = assistent.create_embedding(text)\n",
    "embeddings = [assistent.create_embedding(sentence) for sentence in sentences]\n",
    "for i in embeddings:\n",
    "    print(util.pytorch_cos_sim(origin_embedding, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba4c651cdc5ec17c511bce19f294202fdc45305f9de1f927c569f32faa173ead"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
